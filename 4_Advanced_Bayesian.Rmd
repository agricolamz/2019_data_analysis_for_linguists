---
title: "4. Empirical Bayes Estimation. Байесовский доверительный интервал. Bayes Factor"
author: "Г. Мороз"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r, message=FALSE}
library(tidyverse)
```


## 1. Empirical Bayes Estimation

Метод Empirical Bayes estimation --- один из байесовских методов, в рамках которого:

* производят оценку априорного распределения вероятностей на основании имеющихся данных
* используют полученное априорное распределение для получение апостериорной оценки для каждого наблюдения

```{r}
chekhov <- read_tsv("https://raw.githubusercontent.com/agricolamz/2019_data_analysis_for_linguists/master/data/tidy_chekhov.tsv")
chekhov %>% 
  mutate(trunc_titles = str_trunc(titles, 25, side = "right"),
         average = n/n_words) ->
  chekhov
head(chekhov)
```

* 311 рассказов А. Чехова
* число слов в каждом рассказе
* 46610 уникальных слов в каждом рассказе

Наши данные:
```{r}
chekhov %>% 
  filter(word == "не") %>% 
  select(trunc_titles, word, average) %>% 
  ggplot(aes(average)) +
  geom_histogram(fill = "lightblue")+
  geom_density(color = "red")+
  theme_bw()+
  labs(title = 'Частотность слова "не" на основе 305 рассказов А. Чехова')
```

В данном случае, данные можно подогнать под бета распределение $Χ \sim Beta(α_0, β_0)$ (это далеко не всегда так). Подгонку можно осуществлять множеством разных функций, но я воспользуюсь следующей системой уравнений:

$$\mu = \frac{\alpha}{\alpha+\beta}$$
$$\sigma = \frac{\alpha\times\beta}{(\alpha+\beta)^2\times(\alpha+\beta+1)}$$

Из этой системы можно выразить $\alpha$ и $\beta$:

$$\alpha = \left(\frac{1-\mu}{\sigma^2} - \frac{1}{\mu}\right)\times \mu^2$$
$$\beta = \alpha\times\left(\frac{1}{\mu} - 1\right)$$

```{r}
mu <- mean(chekhov$average[chekhov$word == "не"])
var <- var(chekhov$average[chekhov$word == "не"])
alpha0 <- ((1 - mu) / var - 1 / mu) * mu ^ 2
beta0 <- alpha0 * (1 / mu - 1)
alpha0
beta0
```

Посмотрим, насколько хорошо, получившееся распределение подходит к нашим данным:
```{r}
x <- seq(0, 0.1, length = 1000)
estimation <- data_frame(
  x = x,
  density = c(dbeta(x, shape1 = alpha0, shape2 = beta0)))

chekhov %>% 
  filter(word == "не") %>% 
  select(trunc_titles, word, average) %>% 
  ggplot(aes(average)) +
  geom_density(fill = "lightblue")+
  geom_line(data = estimation, aes(x, density))+
  theme_bw()+
  labs(title = 'Частотность слова "не" на основе 305 рассказов А. Чехова',
       subtitle = "черной линией показано бета распределение с α = 5.283022 и β = 231.6328")
```

Полученное распределение можно использовать как априорное распределение для апдейта значений из каждого рассказа. Этот трюк и называется Empirical Bayes estimation.

## 2. Фриквентисткий доверительный интервал
Основная соль фриквинтистского доверительного интервала (по-английски confidence interval) основано на правиле трех сигм нормального распределения:

```{r, echo = FALSE, fig.height= 2.5}
ggplot(data.frame(x = 0:1), aes(x)) + 
        stat_function(fun = dnorm, args = c(0, 1), geom = 'area', xlim = c(-3, 3), fill = 'deepskyblue4') + 
      stat_function(fun = dnorm, args = c(0, 1), geom = 'area', xlim = c(-2, 2), fill = 'cadetblue') + 
    stat_function(fun = dnorm, args = c(0, 1), geom = 'area', xlim = c(-1, 1), fill = 'lightblue') + 
    stat_function(fun = dnorm, args = c(0, 1), xlim = c(-3, 3))+
  theme_bw()+
  geom_line(aes(y = c(0.15), x = c(-1, 1)), arrow = arrow(length=unit(0.2,"cm"), ends="both", type = "closed"))+
  geom_line(aes(y = c(0.03), x = c(-2, 2)), arrow = arrow(length=unit(0.2,"cm"), ends="both", type = "closed"))+
  annotate(geom = "text", x = 0, y = 0.17, label = "68.26%")+
  annotate(geom = "text", x = 0, y = 0.05, label = "95.44%")+
  scale_x_continuous(breaks = c(-3:3))+
  labs(y = "",
       x = "σ")
```

**z-score**:

* 95% данных находится в 1.96 стандартных отклонений
* 99% данных находится в 2.58 стандартных отклонений

Доверительный интервал:

* предположим что данные генеральной совокупности нормально распределены
* тогда доверительные интервалы выборок взятых из генеральной совокупности будут [покрывать среднее генеральной совокупности](https://rpsychologist.com/d3/CI/)

$$\bar{x} \pm z \times \frac{\sigma}{\sqrt{n}}\text{, где } z \text{ — это центральная } 1 - \frac{\alpha}{2} \text{ часть данных}$$

Распространение этой логики на биномиальные данные называется интервал Вальда:

$$\bar{x} = \theta; \sigma = \sqrt{\frac{\theta\times(1-\theta)}{n}}$$

Тогда интервал Вальда:

$$\theta \pm  z\times\sqrt{\frac{\theta\times(1-\theta)} {n}}$$

Есть только одна проблема: работает он плохо. Его аналоги перечислены в других работ:

* assymptotic method with continuity correction
* Wilson score
* Wilson Score method with continuity correction
* Jeffreys interval
* Clopper–Pearson interval (default in R `binom.test()`)
* Agresti–Coull interval
* ... см. пакет `binom`

```{r, fig.height= 7}
chekhov %>% 
  filter(word == "не") %>%
  slice(1:30) %>% 
  group_by(titles) %>% 
  mutate(low_ci = binom.test(x = n, n = n_words)$conf.int[1],
         up_ci = binom.test(x = n, n = n_words)$conf.int[2]) %>%
  ggplot(aes(trunc_titles, average))+
  geom_point()+
  geom_pointrange(aes(ymin = low_ci, ymax = up_ci))+
  theme_bw()+
  coord_flip()+
  labs(title = 'Среднее и 95% CI употребления "не" в рассказах А. Чехова',
       x = "", y = "")
```

В базовом пакете функция `binom.test()` не позволяет выбирать тип доверительного интервала. `ci.method = "Clopper-Pearson"` возможна, если включить библиотеку `mosaic`. 

## 3. Байесовский доверительный интервал
Байесовский доверительный $k$-% интервал (по-английски credible interval) --- это интервал $[\frac{k}{2}, 1-\frac{k}{2}]$ от апостериорного распределения. Давайте используем распределение, полученное в предыдущем разделе в качестве априорного для тридцати рассказов Чехова:

```{r, fig.height=7}
chekhov %>% 
  filter(word == "не") %>%
  slice(1:30) %>% 
  group_by(titles) %>% 
  mutate(alpha_post = n+alpha0,
         beta_post = n_words-n+beta0,
         average_post = alpha_post/(alpha_post+beta_post),
         cred_int_l = qbeta(.025, alpha_post, beta_post),
         cred_int_h = qbeta(.975, alpha_post, beta_post)) ->
  posterior

posterior %>% 
  select(titles, n_words, average, average_post) %>% 
  arrange(n_words)

posterior %>% 
  ggplot(aes(trunc_titles, average_post, ymin = cred_int_l, ymax = cred_int_h))+
  geom_pointrange()+
  coord_flip()+
  theme_bw()
```

```{r, echo= FALSE, fig.height=7}
chekhov %>% 
  filter(word == "не") %>%
  slice(1:30) %>% 
  group_by(titles) %>% 
  mutate(low_ci = binom.test(x = n, n = n_words)$conf.int[1],
         up_ci = binom.test(x = n, n = n_words)$conf.int[2],
         interval = "confidence") %>% 
  ungroup() %>% 
  select(trunc_titles, low_ci, up_ci, interval, average)->
  df_1

chekhov %>% 
  filter(word == "не") %>%
  slice(1:30) %>% 
  group_by(titles) %>% 
  mutate(alpha_post = n+alpha0,
         beta_post = n_words-n+beta0,
         average = alpha_post/(alpha_post+beta_post),
         low_ci = qbeta(.025, alpha_post, beta_post),
         up_ci = qbeta(.975, alpha_post, beta_post),
         interval = "credible") %>% 
  ungroup() %>% 
  select(trunc_titles, low_ci, up_ci, interval, average)->
  df_2

rbind(df_1, df_2) %>% 
  ggplot(aes(trunc_titles, y = average, ymin = low_ci, ymax = up_ci, color = interval)) +
  geom_errorbar()+
  geom_point()+
  coord_flip()+
  theme_bw()+
  xlab("")
```

## 4. Bayes Factor

### 4.1 Формула Байеса опять

$$P(θ|Data) = \frac{P(Data|θ)\times P(θ)}{P(Data)}$$

$$\frac{P(θ|Data)}{P(θ)} = \frac{P(Data|θ)}{P(Data)}$$

Левая часть этого уравнения описывает вероятности относительно параметров, и эти вероятности представляют собой наши представления. Доля описывает, как наши представления относительно параметра θ обновляются в свете данных.

Байесовский фактор берется из этой же формулы:

$$\frac{\frac{P(M_A|Data)}{P(M_A)}}{\frac{P(M_B|Data)}{P(M_B)}} = \frac{\frac{P(Data|M_A)}{P(Data)}}{\frac{P(Data|M_B)}{P(Data)}} = \frac{P(Data|M_A)}{P(Data|M_B)} = BF_{AB}$$

Т. е. байесовский фактор по сути это всего лишь пропорция составленная из двух функций правдоподобия.

[В датасете c грибами](https://raw.githubusercontent.com/agricolamz/2019_BayesDan_winter/master/datasets/mushrooms.csv) (взят c [kaggle](https://www.kaggle.com/uciml/mushroom-classification)) представлено следующее распределение по месту обитания:

```{r}
df <- read_csv("https://github.com/agricolamz/2019_BayesDan_winter/blob/master/datasets/mushrooms.csv?raw=true")
df %>% 
  count(class, habitat) %>% 
  group_by(class) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot(aes(class, prop, fill = habitat, label = round(prop, 3)))+
  geom_col()+
  geom_text(position = position_stack(vjust = 0.5), color = "white")
```

Мы нашли некоторый новый вид грибов на лужайке (`grasses`), а потом в лесу (`woods`). Давайте посчитаем $BF_{edible\ poisonous}$:


$$L(grasses,\ wood|edible) = 0.335 \times 0.447 = 0.149745$$

$$L(grasses,\ wood|poisonous) = 0.189 \times 0.324 = 0.061236$$

$$BF_{edible\ poisonous} = \frac{L(grasses,\ wood|edible)}{L(grasses,\ wood|poisonous)} = \frac{0.149745}{0.061236} = 2.445375$$

### 4.2 
Вашего друга похитили а на почту отправили [датасет](https://raw.githubusercontent.com/agricolamz/2019_BayesDan_winter/master/datasets/weather.csv), в котором записаны данные о погоде из пяти городов. Ваш телефон зазвонил, и друг сказал, что не знает куда его похитили, но за окном легкий дождь (`Rain`). А на следующий день --- сильный дождь (`Rain Thunderstorm`). Посчитайте $BH_{San\_Diego\ Auckland}$ с точностью до 1 знака после запятой.

```{r, include=FALSE}
df <- read.csv("https://raw.githubusercontent.com/agricolamz/2019_BayesDan_winter/master/datasets/weather.csv")
df %>%
  count(city, events) %>% 
  group_by(city) %>% 
  mutate(prop = n/sum(n)) %>% 
  ggplot(aes(city, prop, fill = events, label = round(prop, 3)))+
  geom_col()+
  geom_text(position = position_stack(vjust = 0.5), color = "white")

df %>%
  count(city, events) %>% 
  group_by(city) %>% 
  mutate(prop = n/sum(n)) %>% 
  select(-n) %>% 
  spread(events, prop, fill = 0) %>% 
  mutate(likelihood_r_rt = `Rain , Thunderstorm` * Rain) ->
  results

BF <- round(results$likelihood_r_rt[5]/results$likelihood_r_rt[1], 3)
```

<form name="FormOne" onsubmit="return validateFormOne()" method="post">
<input type="text" name="answerOne">
<input type="submit" value="check">
</form><br>

### 4.3 [Интерпретация байесовского фактора](https://en.wikipedia.org/wiki/Bayes_factor#Interpretation)

<script>
function validateFormOne() {
    var x = document.forms["FormOne"]["answerOne"].value;
    if (x != "1.2") {
        alert("У меня другой ответ...");
        return false;
    } else {
        alert("Да, все правильно");
        return false;
    }
}
</script>
